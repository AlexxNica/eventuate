akka {
  actor {
    serializers {
      eventuate-durable-event = "com.rbmhtechnology.eventuate.serializer.DurableEventSerializer"
      eventuate-replication-filter = "com.rbmhtechnology.eventuate.serializer.ReplicationFilterSerializer"
      eventuate-replication-protocol = "com.rbmhtechnology.eventuate.serializer.ReplicationProtocolSerializer"
    }

    serialization-bindings {
      "com.rbmhtechnology.eventuate.DurableEvent" = eventuate-durable-event
      "com.rbmhtechnology.eventuate.DurableEventBatch" = eventuate-durable-event
      "com.rbmhtechnology.eventuate.ReplicationFilter$Format" = eventuate-replication-filter
      "com.rbmhtechnology.eventuate.ReplicationProtocol$Format" = eventuate-replication-protocol
    }
  }
}

eventuate {
  log.write {
    # Maximum write batch size for emitted events by event-sourced actors.
    batch-size-max = 200
  }

  log.replication {
    # Maximum batch size for event replication and event writing to the target
    # event log.
    batch-size-max = 200

    # Event replication retry interval. Event replication is re-tried at this
    # interval if previous transfer batch was empty.
    retry-interval = 5s

    # Maximum duration of missing heartbeats from a remote location until
    # that location is considered unavailable.
    failure-detection-limit = 60s
  }

  log.leveldb {
    # Root directory for storing the log directories of individual event logs.
    dir = target

    # Use fsync on write.
    fsync = on

    write-dispatcher {
      executor = "thread-pool-executor"
      type = PinnedDispatcher
    }

    read-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-max = 8
      }
    }
  }

  log.kafka {
    # Default partition to use when writing to and reading from a log.
    partition = 0

    consumer {
      # -------------------------------------------------------------------
      # Simple consumer configuration (used for event and metadata reads).
      #
      # See http://kafka.apache.org/documentation.html#consumerconfigs
      # See http://kafka.apache.org/documentation.html#simpleconsumerapi
      # -------------------------------------------------------------------

      socket.timeout.ms = 30000

      socket.receive.buffer.bytes = 65536

      fetch.message.max.bytes = 1048576
    }

    producer {
      # -------------------------------------------------------------------
      # New Kafka producer configuration.
      #
      # See http://kafka.apache.org/documentation.html#newproducerconfigs
      #
      # The bootstrap.servers property is set dynamically by the event
      # log. No need to set it here.
      # -------------------------------------------------------------------

      acks = 1

      reconnect.backoff.ms = 1000

      # Add further Kafka producer settings here, if needed.
      # ...
    }

    zookeeper {
      connect = "localhost:2181"

      session.timeout.ms = 6000

      connection.timeout.ms = 6000

      sync.time.ms = 2000
    }

    write-dispatcher {
      executor = "thread-pool-executor"
      type = PinnedDispatcher
    }

    read-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-max = 8
      }
    }
  }
}
